{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/mnk2978/.local/lib/python3.8/site-packages (0.10.27)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from wandb) (2.24.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (3.20.1)\n",
      "Requirement already satisfied: Click>=7.0 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (8.1.3)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (1.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from wandb) (2.8.1)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: PyYAML in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from wandb) (5.3.1)\n",
      "Requirement already satisfied: subprocess32>=3.5.3 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (3.5.4)\n",
      "Requirement already satisfied: configparser>=3.8.1 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (5.2.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: six>=1.13.0 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from wandb) (1.15.0)\n",
      "Requirement already satisfied: sentry-sdk>=0.4.0 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (1.5.12)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /home/mnk2978/.local/lib/python3.8/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (1.25.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /share/apps/python/3.8.6/intel/lib/python3.8/site-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/mnk2978/.local/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/mnk2978/.local/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torchsummary\n",
      "  Using cached torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 22.1 is available.\n",
      "You should consider upgrading via the '/share/apps/python/3.8.6/intel/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmrunal\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.16 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.27<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Resnet50</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/mrunal/TinyImagenet\" target=\"_blank\">https://wandb.ai/mrunal/TinyImagenet</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/mrunal/TinyImagenet/runs/3b9tleac\" target=\"_blank\">https://wandb.ai/mrunal/TinyImagenet/runs/3b9tleac</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/mnk2978/hpml/finalproj/datamodelparallel/wandb/run-20220517_133658-3b9tleac</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h1>Run(3b9tleac)</h1><iframe src=\"https://wandb.ai/mrunal/TinyImagenet/runs/3b9tleac\" style=\"border:none;width:100%;height:400px\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x14fb025ee790>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Some helper functions for PyTorch, including:\n",
    "    - get_mean_and_std: calculate the mean and std value of dataset.\n",
    "    - msr_init: net parameter initialization.\n",
    "    - progress_bar: progress bar mimic xlua.progress.\n",
    "'''\n",
    "!pip3 install wandb\n",
    "!pip3 install torchsummary\n",
    "\n",
    "#import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import shutil \n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "import wandb\n",
    "import warnings\n",
    "import torch\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import argparse\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from torchsummary import summary\n",
    "import numpy as np\n",
    "wandb.init(project=\"TinyImagenet\", name=\"Resnet50\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_std(dataset):\n",
    "    '''Compute the mean and std value of dataset.'''\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n",
    "    mean = torch.zeros(3)\n",
    "    std = torch.zeros(3)\n",
    "    print('==> Computing mean and std..')\n",
    "    for inputs, targets in dataloader:\n",
    "        for i in range(3):\n",
    "            mean[i] += inputs[:,i,:,:].mean()\n",
    "            std[i] += inputs[:,i,:,:].std()\n",
    "    mean.div_(len(dataset))\n",
    "    std.div_(len(dataset))\n",
    "    return mean, std\n",
    "\n",
    "def init_params(net):\n",
    "    '''Init layer parameters.'''\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal(m.weight, mode='fan_out')\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            init.constant(m.weight, 1)\n",
    "            init.constant(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            init.normal(m.weight, std=1e-3)\n",
    "            if m.bias:\n",
    "                init.constant(m.bias, 0)\n",
    "\n",
    "_, term_width = shutil.get_terminal_size()\n",
    "#_, term_width = os.popen('stty size', 'r').read().split()\n",
    "term_width = int(term_width)\n",
    "\n",
    "TOTAL_BAR_LENGTH = 65.\n",
    "last_time = time.time()\n",
    "begin_time = last_time\n",
    "\n",
    "\n",
    "def progress_bar(current, total, msg=None):\n",
    "    global last_time, begin_time\n",
    "    if current == 0:\n",
    "        begin_time = time.time()  # Reset for new bar.\n",
    "\n",
    "    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n",
    "    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n",
    "\n",
    "    sys.stdout.write(' [')\n",
    "    for i in range(cur_len):\n",
    "        sys.stdout.write('=')\n",
    "    sys.stdout.write('>')\n",
    "    for i in range(rest_len):\n",
    "        sys.stdout.write('.')\n",
    "    sys.stdout.write(']')\n",
    "\n",
    "    cur_time = time.time()\n",
    "    step_time = cur_time - last_time\n",
    "    last_time = cur_time\n",
    "    tot_time = cur_time - begin_time\n",
    "\n",
    "    L = []\n",
    "    L.append('  Step: %s' % format_time(step_time))\n",
    "    L.append(' | Tot: %s' % format_time(tot_time))\n",
    "    if msg:\n",
    "        L.append(' | ' + msg)\n",
    "\n",
    "    msg = ''.join(L)\n",
    "    sys.stdout.write(msg)\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n",
    "        sys.stdout.write(' ')\n",
    "\n",
    "    # Go back to the center of the bar.\n",
    "    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n",
    "        sys.stdout.write('\\b')\n",
    "    sys.stdout.write(' %d/%d ' % (current+1, total))\n",
    "\n",
    "    if current < total-1:\n",
    "        sys.stdout.write('\\r')\n",
    "    else:\n",
    "        sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def format_time(seconds):\n",
    "    days = int(seconds / 3600/24)\n",
    "    seconds = seconds - days*3600*24\n",
    "    hours = int(seconds / 3600)\n",
    "    seconds = seconds - hours*3600\n",
    "    minutes = int(seconds / 60)\n",
    "    seconds = seconds - minutes*60\n",
    "    secondsf = int(seconds)\n",
    "    seconds = seconds - secondsf\n",
    "    millis = int(seconds*1000)\n",
    "\n",
    "    f = ''\n",
    "    i = 1\n",
    "    if days > 0:\n",
    "        f += str(days) + 'D'\n",
    "        i += 1\n",
    "    if hours > 0 and i <= 2:\n",
    "        f += str(hours) + 'h'\n",
    "        i += 1\n",
    "    if minutes > 0 and i <= 2:\n",
    "        f += str(minutes) + 'm'\n",
    "        i += 1\n",
    "    if secondsf > 0 and i <= 2:\n",
    "        f += str(secondsf) + 's'\n",
    "        i += 1\n",
    "    if millis > 0 and i <= 2:\n",
    "        f += str(millis) + 'ms'\n",
    "        i += 1\n",
    "    if f == '':\n",
    "        f = '0ms'\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "==> Building model..\n",
      "<class 'torchvision.models.resnet.ResNet'>\n",
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
      "  (fc): Linear(in_features=2048, out_features=200, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pkgpath = './'\n",
    "save_path = './results/'\n",
    "\n",
    "if os.path.isdir(save_path) == False:\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "sys.path.append(pkgpath)\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_acc = 0  # best test accuracy\n",
    "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(64, padding=8),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_dir = '/scratch/mnk2978/hpml/finalproj_old/tiny-imagenet-200/train'\n",
    "\n",
    "trainset = torchvision.datasets.ImageFolder(\n",
    "    train_dir, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=100, shuffle=True, num_workers=0)\n",
    "\n",
    "test_dir = '/scratch/mnk2978/hpml/finalproj_old/tiny-imagenet-200/val/images'\n",
    "testset = torchvision.datasets.ImageFolder(\n",
    "    test_dir, transform=transform_test) \n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=0)\n",
    "classes = 200\n",
    "img_size = 64\n",
    "\n",
    "# Model\n",
    "print('==> Building model..')\n",
    "import torchvision.models as models\n",
    "\n",
    "from torchvision.models.resnet import ResNet, Bottleneck\n",
    "\n",
    "num_classes = 200\n",
    "\n",
    "print(ResNet)\n",
    "    \n",
    "net = models.resnet50()\n",
    "\n",
    "\n",
    "#Finetune Final few layers to adjust for tiny imagenet input\n",
    "net.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "net.fc.out_features = 200\n",
    "print\n",
    "\n",
    "net = net.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1,\n",
    "                      momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "print(net)\n",
    "#%%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0/90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mnk2978/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1033: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [================================================================>]  Step: 1s820ms | Tot: 3m20s | train_Loss: 5.387 | train_Acc: 1.377% (1377/100000) 1000/1000 ..........]  Step: 204ms | Tot: 34s624ms | train_Loss: 6.414 | train_Acc: 0.739% (130/17600) 176/1000 ................]  Step: 205ms | Tot: 38s458ms | train_Loss: 6.318 | train_Acc: 0.738% (144/19500) 195/1000 ============================>....................................]  Step: 204ms | Tot: 1m28s | train_Loss: 5.744 | train_Acc: 0.872% (389/44600) 446/1000 \n",
      ">>>best acc: 0, mean: 1.0, std: 0.0................................]  Step: 274ms | Tot: 1ms | test_Loss: 4.407 | test_Acc: 1.000% (1/100) 1/100 \n",
      ">>>best acc: 0, mean: 0.75, std: 0.25..............................]  Step: 165ms | Tot: 166ms | test_Loss: 4.770 | test_Acc: 0.500% (1/200) 2/100 \n",
      ">>>best acc: 0, mean: 0.61, std: 0.28..............................]  Step: 149ms | Tot: 315ms | test_Loss: 4.881 | test_Acc: 0.333% (1/300) 3/100 \n",
      ">>>best acc: 0, mean: 0.52, std: 0.29..............................]  Step: 137ms | Tot: 452ms | test_Loss: 4.903 | test_Acc: 0.250% (1/400) 4/100 \n",
      ">>>best acc: 0, mean: 0.46, std: 0.29..............................]  Step: 151ms | Tot: 603ms | test_Loss: 4.934 | test_Acc: 0.200% (1/500) 5/100 \n",
      ">>>best acc: 0, mean: 0.41, std: 0.29..............................]  Step: 147ms | Tot: 750ms | test_Loss: 4.957 | test_Acc: 0.167% (1/600) 6/100 \n",
      ">>>best acc: 0, mean: 0.96, std: 1.38..............................]  Step: 147ms | Tot: 898ms | test_Loss: 4.799 | test_Acc: 4.286% (30/700) 7/100 \n",
      ">>>best acc: 0, mean: 1.33, std: 1.61..............................]  Step: 150ms | Tot: 1s49ms | test_Loss: 4.829 | test_Acc: 3.875% (31/800) 8/100 \n",
      ">>>best acc: 0, mean: 1.56, std: 1.66..............................]  Step: 145ms | Tot: 1s194ms | test_Loss: 4.893 | test_Acc: 3.444% (31/900) 9/100 \n",
      ">>>best acc: 0, mean: 1.72, std: 1.64..............................]  Step: 146ms | Tot: 1s340ms | test_Loss: 4.907 | test_Acc: 3.100% (31/1000) 10/100 \n",
      ">>>best acc: 0, mean: 1.96, std: 1.74..............................]  Step: 149ms | Tot: 1s490ms | test_Loss: 4.934 | test_Acc: 4.364% (48/1100) 11/100 \n",
      ">>>best acc: 0, mean: 2.13, std: 1.76..............................]  Step: 145ms | Tot: 1s635ms | test_Loss: 4.899 | test_Acc: 4.000% (48/1200) 12/100 \n",
      ">>>best acc: 0, mean: 2.25, std: 1.74..............................]  Step: 147ms | Tot: 1s783ms | test_Loss: 4.904 | test_Acc: 3.692% (48/1300) 13/100 \n",
      ">>>best acc: 0, mean: 2.33, std: 1.7...............................]  Step: 142ms | Tot: 1s926ms | test_Loss: 4.906 | test_Acc: 3.429% (48/1400) 14/100 \n",
      ">>>best acc: 0, mean: 2.39, std: 1.66..............................]  Step: 149ms | Tot: 2s75ms | test_Loss: 4.911 | test_Acc: 3.200% (48/1500) 15/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 1.62..............................]  Step: 168ms | Tot: 2s244ms | test_Loss: 4.941 | test_Acc: 3.000% (48/1600) 16/100 \n",
      ">>>best acc: 0, mean: 2.45, std: 1.57..............................]  Step: 148ms | Tot: 2s392ms | test_Loss: 4.957 | test_Acc: 2.824% (48/1700) 17/100 \n",
      ">>>best acc: 0, mean: 2.46, std: 1.53..............................]  Step: 143ms | Tot: 2s535ms | test_Loss: 4.974 | test_Acc: 2.667% (48/1800) 18/100 \n",
      ">>>best acc: 0, mean: 2.47, std: 1.49..............................]  Step: 143ms | Tot: 2s678ms | test_Loss: 4.995 | test_Acc: 2.526% (48/1900) 19/100 \n",
      ">>>best acc: 0, mean: 2.46, std: 1.45..............................]  Step: 145ms | Tot: 2s824ms | test_Loss: 4.993 | test_Acc: 2.400% (48/2000) 20/100 \n",
      ">>>best acc: 0, mean: 2.45, std: 1.41..............................]  Step: 148ms | Tot: 2s973ms | test_Loss: 5.009 | test_Acc: 2.286% (48/2100) 21/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 1.38..............................]  Step: 147ms | Tot: 3s121ms | test_Loss: 5.001 | test_Acc: 2.182% (48/2200) 22/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 1.35..............................]  Step: 161ms | Tot: 3s283ms | test_Loss: 4.963 | test_Acc: 2.435% (56/2300) 23/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 1.32..............................]  Step: 142ms | Tot: 3s426ms | test_Loss: 4.984 | test_Acc: 2.333% (56/2400) 24/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 1.3...............................]  Step: 147ms | Tot: 3s573ms | test_Loss: 4.994 | test_Acc: 2.240% (56/2500) 25/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 1.27..............................]  Step: 144ms | Tot: 3s717ms | test_Loss: 4.995 | test_Acc: 2.154% (56/2600) 26/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 1.25..............................]  Step: 151ms | Tot: 3s868ms | test_Loss: 4.989 | test_Acc: 2.333% (63/2700) 27/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 1.23..............................]  Step: 146ms | Tot: 4s15ms | test_Loss: 4.977 | test_Acc: 2.500% (70/2800) 28/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 1.21..............................]  Step: 167ms | Tot: 4s182ms | test_Loss: 4.970 | test_Acc: 2.517% (73/2900) 29/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 1.19..............................]  Step: 149ms | Tot: 4s332ms | test_Loss: 4.989 | test_Acc: 2.433% (73/3000) 30/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 1.17..............................]  Step: 148ms | Tot: 4s480ms | test_Loss: 4.985 | test_Acc: 2.355% (73/3100) 31/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 1.15..............................]  Step: 147ms | Tot: 4s628ms | test_Loss: 4.993 | test_Acc: 2.281% (73/3200) 32/100 \n",
      ">>>best acc: 0, mean: 2.41, std: 1.13..............................]  Step: 143ms | Tot: 4s771ms | test_Loss: 5.000 | test_Acc: 2.212% (73/3300) 33/100 \n",
      ">>>best acc: 0, mean: 2.4, std: 1.12...............................]  Step: 146ms | Tot: 4s917ms | test_Loss: 5.003 | test_Acc: 2.147% (73/3400) 34/100 \n",
      ">>>best acc: 0, mean: 2.39, std: 1.1...............................]  Step: 147ms | Tot: 5s65ms | test_Loss: 4.998 | test_Acc: 2.086% (73/3500) 35/100 \n",
      ">>>best acc: 0, mean: 2.38, std: 1.09..............................]  Step: 146ms | Tot: 5s212ms | test_Loss: 4.984 | test_Acc: 2.028% (73/3600) 36/100 \n",
      ">>>best acc: 0, mean: 2.37, std: 1.07..............................]  Step: 144ms | Tot: 5s356ms | test_Loss: 4.988 | test_Acc: 1.973% (73/3700) 37/100 \n",
      ">>>best acc: 0, mean: 2.36, std: 1.06..............................]  Step: 142ms | Tot: 5s499ms | test_Loss: 4.993 | test_Acc: 1.974% (75/3800) 38/100 \n",
      ">>>best acc: 0, mean: 2.36, std: 1.05..............................]  Step: 145ms | Tot: 5s644ms | test_Loss: 4.980 | test_Acc: 2.462% (96/3900) 39/100 \n",
      ">>>best acc: 0, mean: 2.36, std: 1.04..............................]  Step: 149ms | Tot: 5s793ms | test_Loss: 4.989 | test_Acc: 2.400% (96/4000) 40/100 \n",
      ">>>best acc: 0, mean: 2.37, std: 1.02..............................]  Step: 156ms | Tot: 5s950ms | test_Loss: 4.988 | test_Acc: 2.439% (100/4100) 41/100 \n",
      ">>>best acc: 0, mean: 2.37, std: 1.01..............................]  Step: 141ms | Tot: 6s92ms | test_Loss: 4.982 | test_Acc: 2.381% (100/4200) 42/100 \n",
      ">>>best acc: 0, mean: 2.37, std: 1.0...............................]  Step: 158ms | Tot: 6s250ms | test_Loss: 4.982 | test_Acc: 2.326% (100/4300) 43/100 \n",
      ">>>best acc: 0, mean: 2.36, std: 0.99..............................]  Step: 148ms | Tot: 6s399ms | test_Loss: 4.977 | test_Acc: 2.273% (100/4400) 44/100 \n",
      ">>>best acc: 0, mean: 2.36, std: 0.98..............................]  Step: 146ms | Tot: 6s545ms | test_Loss: 4.974 | test_Acc: 2.222% (100/4500) 45/100 \n",
      ">>>best acc: 0, mean: 2.36, std: 0.97..............................]  Step: 145ms | Tot: 6s691ms | test_Loss: 4.971 | test_Acc: 2.239% (103/4600) 46/100 \n",
      ">>>best acc: 0, mean: 2.35, std: 0.96..............................]  Step: 147ms | Tot: 6s839ms | test_Loss: 4.972 | test_Acc: 2.191% (103/4700) 47/100 \n",
      ">>>best acc: 0, mean: 2.35, std: 0.95..............................]  Step: 152ms | Tot: 6s991ms | test_Loss: 4.972 | test_Acc: 2.333% (112/4800) 48/100 \n",
      ">>>best acc: 0, mean: 2.35, std: 0.94..............................]  Step: 147ms | Tot: 7s139ms | test_Loss: 4.965 | test_Acc: 2.286% (112/4900) 49/100 \n",
      ">>>best acc: 0, mean: 2.35, std: 0.93..............................]  Step: 150ms | Tot: 7s289ms | test_Loss: 4.961 | test_Acc: 2.400% (120/5000) 50/100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>best acc: 0, mean: 2.36, std: 0.92..............................]  Step: 153ms | Tot: 7s443ms | test_Loss: 4.956 | test_Acc: 2.765% (141/5100) 51/100 \n",
      ">>>best acc: 0, mean: 2.37, std: 0.91..............................]  Step: 149ms | Tot: 7s592ms | test_Loss: 4.955 | test_Acc: 2.712% (141/5200) 52/100 \n",
      ">>>best acc: 0, mean: 2.37, std: 0.9...............................]  Step: 144ms | Tot: 7s736ms | test_Loss: 4.961 | test_Acc: 2.660% (141/5300) 53/100 \n",
      ">>>best acc: 0, mean: 2.38, std: 0.9>..............................]  Step: 147ms | Tot: 7s884ms | test_Loss: 4.961 | test_Acc: 2.722% (147/5400) 54/100 \n",
      ">>>best acc: 0, mean: 2.39, std: 0.89>.............................]  Step: 158ms | Tot: 8s42ms | test_Loss: 4.971 | test_Acc: 2.673% (147/5500) 55/100 \n",
      ">>>best acc: 0, mean: 2.39, std: 0.88>.............................]  Step: 166ms | Tot: 8s208ms | test_Loss: 4.967 | test_Acc: 2.625% (147/5600) 56/100 \n",
      ">>>best acc: 0, mean: 2.4, std: 0.88==>............................]  Step: 148ms | Tot: 8s357ms | test_Loss: 4.957 | test_Acc: 2.737% (156/5700) 57/100 \n",
      ">>>best acc: 0, mean: 2.4, std: 0.87===>...........................]  Step: 149ms | Tot: 8s507ms | test_Loss: 4.958 | test_Acc: 2.707% (157/5800) 58/100 \n",
      ">>>best acc: 0, mean: 2.41, std: 0.86==>...........................]  Step: 149ms | Tot: 8s657ms | test_Loss: 4.958 | test_Acc: 2.678% (158/5900) 59/100 \n",
      ">>>best acc: 0, mean: 2.41, std: 0.86===>..........................]  Step: 150ms | Tot: 8s807ms | test_Loss: 4.953 | test_Acc: 2.750% (165/6000) 60/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 0.85====>.........................]  Step: 144ms | Tot: 8s952ms | test_Loss: 4.948 | test_Acc: 2.721% (166/6100) 61/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 0.84====>.........................]  Step: 146ms | Tot: 9s98ms | test_Loss: 4.946 | test_Acc: 2.677% (166/6200) 62/100 \n",
      ">>>best acc: 0, mean: 2.42, std: 0.84=====>........................]  Step: 146ms | Tot: 9s245ms | test_Loss: 4.947 | test_Acc: 2.635% (166/6300) 63/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.83=====>........................]  Step: 155ms | Tot: 9s401ms | test_Loss: 4.952 | test_Acc: 2.594% (166/6400) 64/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.82======>.......................]  Step: 149ms | Tot: 9s550ms | test_Loss: 4.963 | test_Acc: 2.554% (166/6500) 65/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.82=======>......................]  Step: 146ms | Tot: 9s696ms | test_Loss: 4.967 | test_Acc: 2.515% (166/6600) 66/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.81=======>......................]  Step: 150ms | Tot: 9s847ms | test_Loss: 4.960 | test_Acc: 2.478% (166/6700) 67/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.81========>.....................]  Step: 147ms | Tot: 9s995ms | test_Loss: 4.963 | test_Acc: 2.441% (166/6800) 68/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.8==========>....................]  Step: 154ms | Tot: 10s150ms | test_Loss: 4.968 | test_Acc: 2.406% (166/6900) 69/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.8==========>....................]  Step: 180ms | Tot: 10s331ms | test_Loss: 4.979 | test_Acc: 2.371% (166/7000) 70/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.79==========>...................]  Step: 141ms | Tot: 10s472ms | test_Loss: 4.985 | test_Acc: 2.338% (166/7100) 71/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.78===========>..................]  Step: 152ms | Tot: 10s625ms | test_Loss: 4.988 | test_Acc: 2.306% (166/7200) 72/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.78===========>..................]  Step: 145ms | Tot: 10s770ms | test_Loss: 4.983 | test_Acc: 2.384% (174/7300) 73/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.77============>.................]  Step: 147ms | Tot: 10s918ms | test_Loss: 4.980 | test_Acc: 2.527% (187/7400) 74/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.77=============>................]  Step: 150ms | Tot: 11s69ms | test_Loss: 4.975 | test_Acc: 2.493% (187/7500) 75/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.76=============>................]  Step: 147ms | Tot: 11s216ms | test_Loss: 4.981 | test_Acc: 2.461% (187/7600) 76/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.76==============>...............]  Step: 154ms | Tot: 11s371ms | test_Loss: 4.985 | test_Acc: 2.429% (187/7700) 77/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.75===============>..............]  Step: 146ms | Tot: 11s517ms | test_Loss: 4.983 | test_Acc: 2.410% (188/7800) 78/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.75===============>..............]  Step: 147ms | Tot: 11s664ms | test_Loss: 4.978 | test_Acc: 2.519% (199/7900) 79/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.74================>.............]  Step: 146ms | Tot: 11s811ms | test_Loss: 4.979 | test_Acc: 2.487% (199/8000) 80/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.74=================>............]  Step: 153ms | Tot: 11s965ms | test_Loss: 4.979 | test_Acc: 2.457% (199/8100) 81/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.74=================>............]  Step: 148ms | Tot: 12s114ms | test_Loss: 4.982 | test_Acc: 2.427% (199/8200) 82/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.73==================>...........]  Step: 170ms | Tot: 12s284ms | test_Loss: 4.984 | test_Acc: 2.398% (199/8300) 83/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.73==================>...........]  Step: 153ms | Tot: 12s438ms | test_Loss: 4.977 | test_Acc: 2.571% (216/8400) 84/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.72===================>..........]  Step: 145ms | Tot: 12s583ms | test_Loss: 4.980 | test_Acc: 2.553% (217/8500) 85/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.72====================>.........]  Step: 145ms | Tot: 12s729ms | test_Loss: 4.981 | test_Acc: 2.523% (217/8600) 86/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.71====================>.........]  Step: 151ms | Tot: 12s880ms | test_Loss: 4.979 | test_Acc: 2.494% (217/8700) 87/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.71=====================>........]  Step: 150ms | Tot: 13s31ms | test_Loss: 4.984 | test_Acc: 2.466% (217/8800) 88/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.71======================>.......]  Step: 150ms | Tot: 13s182ms | test_Loss: 4.986 | test_Acc: 2.438% (217/8900) 89/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.7=======================>.......]  Step: 151ms | Tot: 13s333ms | test_Loss: 4.984 | test_Acc: 2.411% (217/9000) 90/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.7========================>......]  Step: 150ms | Tot: 13s483ms | test_Loss: 4.982 | test_Acc: 2.385% (217/9100) 91/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.69========================>.....]  Step: 150ms | Tot: 13s634ms | test_Loss: 4.979 | test_Acc: 2.391% (220/9200) 92/100 \n",
      ">>>best acc: 0, mean: 2.43, std: 0.69========================>.....]  Step: 147ms | Tot: 13s782ms | test_Loss: 4.979 | test_Acc: 2.366% (220/9300) 93/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.69=========================>....]  Step: 150ms | Tot: 13s933ms | test_Loss: 4.961 | test_Acc: 2.702% (254/9400) 94/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.68==========================>...]  Step: 166ms | Tot: 14s99ms | test_Loss: 4.954 | test_Acc: 2.674% (254/9500) 95/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.68==========================>...]  Step: 152ms | Tot: 14s251ms | test_Loss: 4.951 | test_Acc: 2.646% (254/9600) 96/100 \n",
      ">>>best acc: 0, mean: 2.44, std: 0.68===========================>..]  Step: 185ms | Tot: 14s437ms | test_Loss: 4.945 | test_Acc: 2.629% (255/9700) 97/100 \n",
      ">>>best acc: 0, mean: 2.45, std: 0.68============================>.]  Step: 149ms | Tot: 14s587ms | test_Loss: 4.938 | test_Acc: 2.755% (270/9800) 98/100 \n",
      ">>>best acc: 0, mean: 2.45, std: 0.67============================>.]  Step: 150ms | Tot: 14s738ms | test_Loss: 4.933 | test_Acc: 2.727% (270/9900) 99/100 \n",
      " [================================================================>]  Step: 144ms | Tot: 14s882ms | test_Loss: 4.928 | test_Acc: 2.810% (281/10000) 100/100 \n",
      ">>>best acc: 0, mean: 2.45, std: 0.67\n",
      "Saving..\n",
      ">>>best acc: 2.81\n",
      "Epoch:1/90\n",
      " [================================================================>]  Step: 270ms | Tot: 2m51s | train_Loss: 4.707 | train_Acc: 4.660% (4660/100000) 1000/1000  \n",
      ">>>best acc: 2.81, mean: 18.0, std: 0.0............................]  Step: 124ms | Tot: 1ms | test_Loss: 3.578 | test_Acc: 18.000% (18/100) 1/100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>best acc: 2.81, mean: 13.5, std: 4.5............................]  Step: 115ms | Tot: 116ms | test_Loss: 4.310 | test_Acc: 9.000% (18/200) 2/100 \n",
      ">>>best acc: 2.81, mean: 11.67, std: 4.5...........................]  Step: 115ms | Tot: 231ms | test_Loss: 4.391 | test_Acc: 8.000% (24/300) 3/100 \n",
      ">>>best acc: 2.81, mean: 10.44, std: 4.44..........................]  Step: 116ms | Tot: 347ms | test_Loss: 4.420 | test_Acc: 6.750% (27/400) 4/100 \n",
      ">>>best acc: 2.81, mean: 10.19, std: 4.0...........................]  Step: 114ms | Tot: 462ms | test_Loss: 4.353 | test_Acc: 9.200% (46/500) 5/100 \n",
      ">>>best acc: 2.81, mean: 9.99, std: 3.68...........................]  Step: 113ms | Tot: 576ms | test_Loss: 4.363 | test_Acc: 9.000% (54/600) 6/100 \n",
      ">>>best acc: 2.81, mean: 9.77, std: 3.45...........................]  Step: 113ms | Tot: 689ms | test_Loss: 4.348 | test_Acc: 8.429% (59/700) 7/100 \n",
      ">>>best acc: 2.81, mean: 9.47, std: 3.32...........................]  Step: 104ms | Tot: 793ms | test_Loss: 4.397 | test_Acc: 7.375% (59/800) 8/100 \n",
      ">>>best acc: 2.81, mean: 9.17, std: 3.24...........................]  Step: 133ms | Tot: 927ms | test_Loss: 4.476 | test_Acc: 6.778% (61/900) 9/100 \n",
      ">>>best acc: 2.81, mean: 8.86, std: 3.21...........................]  Step: 113ms | Tot: 1s41ms | test_Loss: 4.462 | test_Acc: 6.100% (61/1000) 10/100 \n",
      ">>>best acc: 2.81, mean: 8.67, std: 3.12...........................]  Step: 108ms | Tot: 1s149ms | test_Loss: 4.481 | test_Acc: 6.727% (74/1100) 11/100 \n",
      ">>>best acc: 2.81, mean: 8.49, std: 3.05...........................]  Step: 109ms | Tot: 1s258ms | test_Loss: 4.485 | test_Acc: 6.500% (78/1200) 12/100 \n",
      ">>>best acc: 2.81, mean: 8.37, std: 2.96...........................]  Step: 113ms | Tot: 1s372ms | test_Loss: 4.457 | test_Acc: 7.000% (91/1300) 13/100 \n",
      ">>>best acc: 2.81, mean: 8.24, std: 2.89...........................]  Step: 115ms | Tot: 1s487ms | test_Loss: 4.492 | test_Acc: 6.500% (91/1400) 14/100 \n",
      ">>>best acc: 2.81, mean: 8.1, std: 2.84............................]  Step: 107ms | Tot: 1s595ms | test_Loss: 4.532 | test_Acc: 6.067% (91/1500) 15/100 \n",
      ">>>best acc: 2.81, mean: 7.94, std: 2.82...........................]  Step: 116ms | Tot: 1s712ms | test_Loss: 4.561 | test_Acc: 5.688% (91/1600) 16/100 \n",
      ">>>best acc: 2.81, mean: 7.81, std: 2.79...........................]  Step: 115ms | Tot: 1s827ms | test_Loss: 4.558 | test_Acc: 5.588% (95/1700) 17/100 \n",
      ">>>best acc: 2.81, mean: 7.67, std: 2.76...........................]  Step: 116ms | Tot: 1s944ms | test_Loss: 4.554 | test_Acc: 5.444% (98/1800) 18/100 \n",
      ">>>best acc: 2.81, mean: 7.56, std: 2.73...........................]  Step: 114ms | Tot: 2s59ms | test_Loss: 4.527 | test_Acc: 5.474% (104/1900) 19/100 \n",
      ">>>best acc: 2.81, mean: 7.44, std: 2.71...........................]  Step: 127ms | Tot: 2s186ms | test_Loss: 4.534 | test_Acc: 5.200% (104/2000) 20/100 \n",
      ">>>best acc: 2.81, mean: 7.32, std: 2.7............................]  Step: 118ms | Tot: 2s305ms | test_Loss: 4.554 | test_Acc: 4.952% (104/2100) 21/100 \n",
      ">>>best acc: 2.81, mean: 7.24, std: 2.67...........................]  Step: 110ms | Tot: 2s416ms | test_Loss: 4.531 | test_Acc: 5.409% (119/2200) 22/100 \n",
      ">>>best acc: 2.81, mean: 7.2, std: 2.61............................]  Step: 111ms | Tot: 2s528ms | test_Loss: 4.486 | test_Acc: 6.522% (150/2300) 23/100 \n",
      ">>>best acc: 2.81, mean: 7.16, std: 2.57...........................]  Step: 118ms | Tot: 2s646ms | test_Loss: 4.525 | test_Acc: 6.250% (150/2400) 24/100 \n",
      ">>>best acc: 2.81, mean: 7.13, std: 2.52...........................]  Step: 115ms | Tot: 2s761ms | test_Loss: 4.528 | test_Acc: 6.240% (156/2500) 25/100 \n",
      ">>>best acc: 2.81, mean: 7.08, std: 2.48...........................]  Step: 114ms | Tot: 2s876ms | test_Loss: 4.526 | test_Acc: 6.000% (156/2600) 26/100 \n",
      ">>>best acc: 2.81, mean: 7.04, std: 2.44...........................]  Step: 139ms | Tot: 3s15ms | test_Loss: 4.517 | test_Acc: 5.889% (159/2700) 27/100 \n",
      ">>>best acc: 2.81, mean: 7.01, std: 2.41...........................]  Step: 111ms | Tot: 3s127ms | test_Loss: 4.501 | test_Acc: 6.107% (171/2800) 28/100 \n",
      ">>>best acc: 2.81, mean: 6.97, std: 2.37...........................]  Step: 119ms | Tot: 3s246ms | test_Loss: 4.507 | test_Acc: 5.897% (171/2900) 29/100 \n",
      ">>>best acc: 2.81, mean: 6.93, std: 2.34...........................]  Step: 114ms | Tot: 3s360ms | test_Loss: 4.513 | test_Acc: 5.700% (171/3000) 30/100 \n",
      ">>>best acc: 2.81, mean: 6.88, std: 2.32...........................]  Step: 117ms | Tot: 3s478ms | test_Loss: 4.498 | test_Acc: 5.613% (174/3100) 31/100 \n",
      ">>>best acc: 2.81, mean: 6.84, std: 2.29...........................]  Step: 125ms | Tot: 3s603ms | test_Loss: 4.493 | test_Acc: 5.594% (179/3200) 32/100 \n",
      ">>>best acc: 2.81, mean: 6.8, std: 2.27............................]  Step: 117ms | Tot: 3s720ms | test_Loss: 4.503 | test_Acc: 5.424% (179/3300) 33/100 \n",
      ">>>best acc: 2.81, mean: 6.76, std: 2.25...........................]  Step: 118ms | Tot: 3s839ms | test_Loss: 4.530 | test_Acc: 5.265% (179/3400) 34/100 \n",
      ">>>best acc: 2.81, mean: 6.71, std: 2.23...........................]  Step: 115ms | Tot: 3s954ms | test_Loss: 4.533 | test_Acc: 5.314% (186/3500) 35/100 \n",
      ">>>best acc: 2.81, mean: 6.67, std: 2.22...........................]  Step: 120ms | Tot: 4s75ms | test_Loss: 4.519 | test_Acc: 5.194% (187/3600) 36/100 \n",
      ">>>best acc: 2.81, mean: 6.63, std: 2.2............................]  Step: 122ms | Tot: 4s197ms | test_Loss: 4.528 | test_Acc: 5.054% (187/3700) 37/100 \n",
      ">>>best acc: 2.81, mean: 6.59, std: 2.19...........................]  Step: 117ms | Tot: 4s315ms | test_Loss: 4.535 | test_Acc: 5.026% (191/3800) 38/100 \n",
      ">>>best acc: 2.81, mean: 6.54, std: 2.18...........................]  Step: 115ms | Tot: 4s431ms | test_Loss: 4.540 | test_Acc: 4.897% (191/3900) 39/100 \n",
      ">>>best acc: 2.81, mean: 6.5, std: 2.16............................]  Step: 119ms | Tot: 4s550ms | test_Loss: 4.542 | test_Acc: 5.000% (200/4000) 40/100 \n",
      ">>>best acc: 2.81, mean: 6.47, std: 2.15...........................]  Step: 114ms | Tot: 4s664ms | test_Loss: 4.541 | test_Acc: 5.049% (207/4100) 41/100 \n",
      ">>>best acc: 2.81, mean: 6.44, std: 2.13...........................]  Step: 123ms | Tot: 4s788ms | test_Loss: 4.526 | test_Acc: 5.143% (216/4200) 42/100 \n",
      ">>>best acc: 2.81, mean: 6.41, std: 2.12...........................]  Step: 120ms | Tot: 4s908ms | test_Loss: 4.530 | test_Acc: 5.093% (219/4300) 43/100 \n",
      ">>>best acc: 2.81, mean: 6.38, std: 2.1............................]  Step: 132ms | Tot: 5s41ms | test_Loss: 4.525 | test_Acc: 5.114% (225/4400) 44/100 \n",
      ">>>best acc: 2.81, mean: 6.35, std: 2.09...........................]  Step: 116ms | Tot: 5s158ms | test_Loss: 4.530 | test_Acc: 5.111% (230/4500) 45/100 \n",
      ">>>best acc: 2.81, mean: 6.33, std: 2.07...........................]  Step: 116ms | Tot: 5s275ms | test_Loss: 4.510 | test_Acc: 5.478% (252/4600) 46/100 \n",
      ">>>best acc: 2.81, mean: 6.31, std: 2.05...........................]  Step: 111ms | Tot: 5s386ms | test_Loss: 4.517 | test_Acc: 5.383% (253/4700) 47/100 \n",
      ">>>best acc: 2.81, mean: 6.29, std: 2.03...........................]  Step: 116ms | Tot: 5s502ms | test_Loss: 4.513 | test_Acc: 5.333% (256/4800) 48/100 \n",
      ">>>best acc: 2.81, mean: 6.27, std: 2.01...........................]  Step: 112ms | Tot: 5s615ms | test_Loss: 4.510 | test_Acc: 5.429% (266/4900) 49/100 \n",
      ">>>best acc: 2.81, mean: 6.25, std: 2.0............................]  Step: 111ms | Tot: 5s727ms | test_Loss: 4.517 | test_Acc: 5.360% (268/5000) 50/100 \n",
      ">>>best acc: 2.81, mean: 6.23, std: 1.98...........................]  Step: 115ms | Tot: 5s842ms | test_Loss: 4.514 | test_Acc: 5.255% (268/5100) 51/100 \n",
      ">>>best acc: 2.81, mean: 6.22, std: 1.97...........................]  Step: 115ms | Tot: 5s958ms | test_Loss: 4.521 | test_Acc: 5.269% (274/5200) 52/100 \n",
      ">>>best acc: 2.81, mean: 6.2, std: 1.95............................]  Step: 112ms | Tot: 6s70ms | test_Loss: 4.524 | test_Acc: 5.245% (278/5300) 53/100 \n",
      ">>>best acc: 2.81, mean: 6.18, std: 1.94...........................]  Step: 115ms | Tot: 6s186ms | test_Loss: 4.534 | test_Acc: 5.167% (279/5400) 54/100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>best acc: 2.81, mean: 6.16, std: 1.93...........................]  Step: 130ms | Tot: 6s316ms | test_Loss: 4.533 | test_Acc: 5.109% (281/5500) 55/100 \n",
      ">>>best acc: 2.81, mean: 6.14, std: 1.92...........................]  Step: 116ms | Tot: 6s433ms | test_Loss: 4.527 | test_Acc: 5.161% (289/5600) 56/100 \n",
      ">>>best acc: 2.81, mean: 6.12, std: 1.9............................]  Step: 122ms | Tot: 6s555ms | test_Loss: 4.524 | test_Acc: 5.088% (290/5700) 57/100 \n",
      ">>>best acc: 2.81, mean: 6.11, std: 1.89...........................]  Step: 121ms | Tot: 6s677ms | test_Loss: 4.516 | test_Acc: 5.397% (313/5800) 58/100 \n",
      ">>>best acc: 2.81, mean: 6.1, std: 1.88>...........................]  Step: 118ms | Tot: 6s796ms | test_Loss: 4.512 | test_Acc: 5.559% (328/5900) 59/100 \n",
      ">>>best acc: 2.81, mean: 6.09, std: 1.86>..........................]  Step: 116ms | Tot: 6s913ms | test_Loss: 4.507 | test_Acc: 5.667% (340/6000) 60/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.85=>.........................]  Step: 149ms | Tot: 7s62ms | test_Loss: 4.513 | test_Acc: 5.574% (340/6100) 61/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.83=>.........................]  Step: 110ms | Tot: 7s173ms | test_Loss: 4.515 | test_Acc: 5.484% (340/6200) 62/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.82==>........................]  Step: 117ms | Tot: 7s290ms | test_Loss: 4.507 | test_Acc: 5.825% (367/6300) 63/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.8===>........................]  Step: 120ms | Tot: 7s411ms | test_Loss: 4.507 | test_Acc: 5.859% (375/6400) 64/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.79===>.......................]  Step: 117ms | Tot: 7s529ms | test_Loss: 4.508 | test_Acc: 5.985% (389/6500) 65/100 \n",
      ">>>best acc: 2.81, mean: 6.06, std: 1.78====>......................]  Step: 111ms | Tot: 7s640ms | test_Loss: 4.511 | test_Acc: 5.894% (389/6600) 66/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.76====>......................]  Step: 116ms | Tot: 7s757ms | test_Loss: 4.494 | test_Acc: 6.343% (425/6700) 67/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.75=====>.....................]  Step: 120ms | Tot: 7s877ms | test_Loss: 4.502 | test_Acc: 6.250% (425/6800) 68/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.74======>....................]  Step: 115ms | Tot: 7s992ms | test_Loss: 4.507 | test_Acc: 6.203% (428/6900) 69/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.73======>....................]  Step: 119ms | Tot: 8s112ms | test_Loss: 4.509 | test_Acc: 6.114% (428/7000) 70/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.71=======>...................]  Step: 115ms | Tot: 8s228ms | test_Loss: 4.510 | test_Acc: 6.042% (429/7100) 71/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.7=========>..................]  Step: 118ms | Tot: 8s346ms | test_Loss: 4.509 | test_Acc: 6.069% (437/7200) 72/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.69========>..................]  Step: 120ms | Tot: 8s467ms | test_Loss: 4.501 | test_Acc: 6.329% (462/7300) 73/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.68=========>.................]  Step: 113ms | Tot: 8s581ms | test_Loss: 4.497 | test_Acc: 6.351% (470/7400) 74/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.67==========>................]  Step: 118ms | Tot: 8s699ms | test_Loss: 4.500 | test_Acc: 6.267% (470/7500) 75/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.66==========>................]  Step: 116ms | Tot: 8s816ms | test_Loss: 4.505 | test_Acc: 6.184% (470/7600) 76/100 \n",
      ">>>best acc: 2.81, mean: 6.09, std: 1.65===========>...............]  Step: 116ms | Tot: 8s933ms | test_Loss: 4.501 | test_Acc: 6.208% (478/7700) 77/100 \n",
      ">>>best acc: 2.81, mean: 6.09, std: 1.64============>..............]  Step: 142ms | Tot: 9s75ms | test_Loss: 4.508 | test_Acc: 6.128% (478/7800) 78/100 \n",
      ">>>best acc: 2.81, mean: 6.09, std: 1.63============>..............]  Step: 135ms | Tot: 9s210ms | test_Loss: 4.515 | test_Acc: 6.051% (478/7900) 79/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.62=============>.............]  Step: 114ms | Tot: 9s324ms | test_Loss: 4.518 | test_Acc: 6.013% (481/8000) 80/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.61==============>............]  Step: 114ms | Tot: 9s439ms | test_Loss: 4.522 | test_Acc: 5.951% (482/8100) 81/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.6===============>............]  Step: 121ms | Tot: 9s560ms | test_Loss: 4.522 | test_Acc: 5.951% (488/8200) 82/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.59===============>...........]  Step: 120ms | Tot: 9s681ms | test_Loss: 4.521 | test_Acc: 6.012% (499/8300) 83/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.58===============>...........]  Step: 116ms | Tot: 9s797ms | test_Loss: 4.514 | test_Acc: 6.167% (518/8400) 84/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.57================>..........]  Step: 122ms | Tot: 9s920ms | test_Loss: 4.512 | test_Acc: 6.165% (524/8500) 85/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.56=================>.........]  Step: 120ms | Tot: 10s41ms | test_Loss: 4.517 | test_Acc: 6.093% (524/8600) 86/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.55=================>.........]  Step: 119ms | Tot: 10s160ms | test_Loss: 4.520 | test_Acc: 6.023% (524/8700) 87/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.54==================>........]  Step: 125ms | Tot: 10s286ms | test_Loss: 4.524 | test_Acc: 5.966% (525/8800) 88/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.53===================>.......]  Step: 118ms | Tot: 10s405ms | test_Loss: 4.520 | test_Acc: 5.933% (528/8900) 89/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.52===================>.......]  Step: 119ms | Tot: 10s524ms | test_Loss: 4.515 | test_Acc: 5.878% (529/9000) 90/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.52====================>......]  Step: 115ms | Tot: 10s640ms | test_Loss: 4.512 | test_Acc: 5.901% (537/9100) 91/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.51=====================>.....]  Step: 116ms | Tot: 10s756ms | test_Loss: 4.503 | test_Acc: 5.924% (545/9200) 92/100 \n",
      ">>>best acc: 2.81, mean: 6.07, std: 1.5======================>.....]  Step: 114ms | Tot: 10s871ms | test_Loss: 4.491 | test_Acc: 6.043% (562/9300) 93/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.49======================>....]  Step: 115ms | Tot: 10s986ms | test_Loss: 4.467 | test_Acc: 6.457% (607/9400) 94/100 \n",
      ">>>best acc: 2.81, mean: 6.08, std: 1.49=======================>...]  Step: 141ms | Tot: 11s128ms | test_Loss: 4.449 | test_Acc: 6.663% (633/9500) 95/100 \n",
      ">>>best acc: 2.81, mean: 6.09, std: 1.48=======================>...]  Step: 124ms | Tot: 11s253ms | test_Loss: 4.431 | test_Acc: 6.979% (670/9600) 96/100 \n",
      ">>>best acc: 2.81, mean: 6.1, std: 1.48=========================>..]  Step: 134ms | Tot: 11s387ms | test_Loss: 4.418 | test_Acc: 7.206% (699/9700) 97/100 \n",
      ">>>best acc: 2.81, mean: 6.12, std: 1.47=========================>.]  Step: 143ms | Tot: 11s531ms | test_Loss: 4.418 | test_Acc: 7.276% (713/9800) 98/100 \n",
      ">>>best acc: 2.81, mean: 6.13, std: 1.47=========================>.]  Step: 148ms | Tot: 11s680ms | test_Loss: 4.407 | test_Acc: 7.505% (743/9900) 99/100 \n",
      " [================================================================>]  Step: 147ms | Tot: 11s827ms | test_Loss: 4.411 | test_Acc: 7.430% (743/10000) 100/100 \n",
      ">>>best acc: 2.81, mean: 6.14, std: 1.47\n",
      "Saving..\n",
      ">>>best acc: 7.43\n",
      "Epoch:2/90\n",
      " [================================================================>]  Step: 239ms | Tot: 2m26s | train_Loss: 4.171 | train_Acc: 11.052% (11052/100000) 1000/1000 \n",
      ">>>best acc: 7.43, mean: 38.0, std: 0.0............................]  Step: 103ms | Tot: 1ms | test_Loss: 3.081 | test_Acc: 38.000% (38/100) 1/100 \n",
      ">>>best acc: 7.43, mean: 31.75, std: 6.25..........................]  Step: 102ms | Tot: 103ms | test_Loss: 3.343 | test_Acc: 25.500% (51/200) 2/100 \n",
      ">>>best acc: 7.43, mean: 27.72, std: 7.65..........................]  Step: 98ms | Tot: 202ms | test_Loss: 3.780 | test_Acc: 19.667% (59/300) 3/100 \n",
      ">>>best acc: 7.43, mean: 25.35, std: 7.79..........................]  Step: 97ms | Tot: 299ms | test_Loss: 3.757 | test_Acc: 18.250% (73/400) 4/100 \n",
      ">>>best acc: 7.43, mean: 23.6, std: 7.8............................]  Step: 97ms | Tot: 397ms | test_Loss: 3.770 | test_Acc: 16.600% (83/500) 5/100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>best acc: 7.43, mean: 21.98, std: 8.0...........................]  Step: 95ms | Tot: 492ms | test_Loss: 3.948 | test_Acc: 13.833% (83/600) 6/100 \n",
      ">>>best acc: 7.43, mean: 21.14, std: 7.68..........................]  Step: 96ms | Tot: 589ms | test_Loss: 3.993 | test_Acc: 16.143% (113/700) 7/100 \n",
      ">>>best acc: 7.43, mean: 20.45, std: 7.41..........................]  Step: 133ms | Tot: 722ms | test_Loss: 3.963 | test_Acc: 15.625% (125/800) 8/100 \n",
      ">>>best acc: 7.43, mean: 19.88, std: 7.17..........................]  Step: 95ms | Tot: 817ms | test_Loss: 3.993 | test_Acc: 15.333% (138/900) 9/100 \n",
      ">>>best acc: 7.43, mean: 19.43, std: 6.94..........................]  Step: 99ms | Tot: 917ms | test_Loss: 3.952 | test_Acc: 15.300% (153/1000) 10/100 \n",
      ">>>best acc: 7.43, mean: 18.96, std: 6.78..........................]  Step: 100ms | Tot: 1s17ms | test_Loss: 4.065 | test_Acc: 14.273% (157/1100) 11/100 \n",
      ">>>best acc: 7.43, mean: 18.57, std: 6.62..........................]  Step: 98ms | Tot: 1s116ms | test_Loss: 4.246 | test_Acc: 14.333% (172/1200) 12/100 \n",
      ">>>best acc: 7.43, mean: 18.3, std: 6.42...........................]  Step: 98ms | Tot: 1s215ms | test_Loss: 4.197 | test_Acc: 15.077% (196/1300) 13/100 \n",
      ">>>best acc: 7.43, mean: 18.0, std: 6.29...........................]  Step: 95ms | Tot: 1s310ms | test_Loss: 4.232 | test_Acc: 14.071% (197/1400) 14/100 \n",
      ">>>best acc: 7.43, mean: 17.7, std: 6.17...........................]  Step: 106ms | Tot: 1s417ms | test_Loss: 4.264 | test_Acc: 13.533% (203/1500) 15/100 \n",
      ">>>best acc: 7.43, mean: 17.43, std: 6.07..........................]  Step: 97ms | Tot: 1s514ms | test_Loss: 4.266 | test_Acc: 13.375% (214/1600) 16/100 \n",
      ">>>best acc: 7.43, mean: 17.16, std: 5.99..........................]  Step: 94ms | Tot: 1s609ms | test_Loss: 4.288 | test_Acc: 12.824% (218/1700) 17/100 \n",
      ">>>best acc: 7.43, mean: 16.92, std: 5.9...........................]  Step: 100ms | Tot: 1s710ms | test_Loss: 4.254 | test_Acc: 12.833% (231/1800) 18/100 \n",
      ">>>best acc: 7.43, mean: 16.77, std: 5.78..........................]  Step: 104ms | Tot: 1s814ms | test_Loss: 4.176 | test_Acc: 14.105% (268/1900) 19/100 \n",
      ">>>best acc: 7.43, mean: 16.61, std: 5.68..........................]  Step: 93ms | Tot: 1s907ms | test_Loss: 4.156 | test_Acc: 13.600% (272/2000) 20/100 \n",
      ">>>best acc: 7.43, mean: 16.45, std: 5.58..........................]  Step: 96ms | Tot: 2s4ms | test_Loss: 4.179 | test_Acc: 13.238% (278/2100) 21/100 \n",
      ">>>best acc: 7.43, mean: 16.28, std: 5.51..........................]  Step: 102ms | Tot: 2s106ms | test_Loss: 4.164 | test_Acc: 12.682% (279/2200) 22/100 \n",
      ">>>best acc: 7.43, mean: 16.19, std: 5.41..........................]  Step: 102ms | Tot: 2s208ms | test_Loss: 4.095 | test_Acc: 14.261% (328/2300) 23/100 \n",
      ">>>best acc: 7.43, mean: 16.11, std: 5.31..........................]  Step: 98ms | Tot: 2s307ms | test_Loss: 4.084 | test_Acc: 14.292% (343/2400) 24/100 \n",
      ">>>best acc: 7.43, mean: 16.03, std: 5.22..........................]  Step: 99ms | Tot: 2s406ms | test_Loss: 4.093 | test_Acc: 14.000% (350/2500) 25/100 \n",
      ">>>best acc: 7.43, mean: 15.95, std: 5.13..........................]  Step: 102ms | Tot: 2s508ms | test_Loss: 4.095 | test_Acc: 13.923% (362/2600) 26/100 \n",
      ">>>best acc: 7.43, mean: 15.89, std: 5.04..........................]  Step: 103ms | Tot: 2s612ms | test_Loss: 4.074 | test_Acc: 14.296% (386/2700) 27/100 \n",
      ">>>best acc: 7.43, mean: 15.82, std: 4.97..........................]  Step: 98ms | Tot: 2s710ms | test_Loss: 4.096 | test_Acc: 13.893% (389/2800) 28/100 \n",
      ">>>best acc: 7.43, mean: 15.75, std: 4.89..........................]  Step: 111ms | Tot: 2s822ms | test_Loss: 4.089 | test_Acc: 13.931% (404/2900) 29/100 \n",
      ">>>best acc: 7.43, mean: 15.69, std: 4.82..........................]  Step: 93ms | Tot: 2s915ms | test_Loss: 4.094 | test_Acc: 13.867% (416/3000) 30/100 \n",
      ">>>best acc: 7.43, mean: 15.65, std: 4.75..........................]  Step: 96ms | Tot: 3s11ms | test_Loss: 4.066 | test_Acc: 14.581% (452/3100) 31/100 \n",
      ">>>best acc: 7.43, mean: 15.62, std: 4.68..........................]  Step: 96ms | Tot: 3s108ms | test_Loss: 4.055 | test_Acc: 14.750% (472/3200) 32/100 \n",
      ">>>best acc: 7.43, mean: 15.58, std: 4.61..........................]  Step: 100ms | Tot: 3s208ms | test_Loss: 4.090 | test_Acc: 14.303% (472/3300) 33/100 \n",
      ">>>best acc: 7.43, mean: 15.54, std: 4.55..........................]  Step: 95ms | Tot: 3s304ms | test_Loss: 4.125 | test_Acc: 14.000% (476/3400) 34/100 \n",
      ">>>best acc: 7.43, mean: 15.49, std: 4.49..........................]  Step: 119ms | Tot: 3s424ms | test_Loss: 4.133 | test_Acc: 13.771% (482/3500) 35/100 \n",
      ">>>best acc: 7.43, mean: 15.44, std: 4.44..........................]  Step: 93ms | Tot: 3s518ms | test_Loss: 4.130 | test_Acc: 13.694% (493/3600) 36/100 \n",
      ">>>best acc: 7.43, mean: 15.39, std: 4.39..........................]  Step: 97ms | Tot: 3s615ms | test_Loss: 4.136 | test_Acc: 13.541% (501/3700) 37/100 \n",
      ">>>best acc: 7.43, mean: 15.34, std: 4.34..........................]  Step: 98ms | Tot: 3s713ms | test_Loss: 4.123 | test_Acc: 13.737% (522/3800) 38/100 \n",
      ">>>best acc: 7.43, mean: 15.3, std: 4.29...........................]  Step: 223ms | Tot: 3s936ms | test_Loss: 4.140 | test_Acc: 13.641% (532/3900) 39/100 \n",
      ">>>best acc: 7.43, mean: 15.27, std: 4.24..........................]  Step: 94ms | Tot: 4s31ms | test_Loss: 4.129 | test_Acc: 14.050% (562/4000) 40/100 \n",
      ">>>best acc: 7.43, mean: 15.23, std: 4.2...........................]  Step: 96ms | Tot: 4s127ms | test_Loss: 4.145 | test_Acc: 13.878% (569/4100) 41/100 \n",
      ">>>best acc: 7.43, mean: 15.2, std: 4.15...........................]  Step: 98ms | Tot: 4s226ms | test_Loss: 4.142 | test_Acc: 13.881% (583/4200) 42/100 \n",
      ">>>best acc: 7.43, mean: 15.17, std: 4.11..........................]  Step: 100ms | Tot: 4s326ms | test_Loss: 4.152 | test_Acc: 13.628% (586/4300) 43/100 \n",
      ">>>best acc: 7.43, mean: 15.13, std: 4.07..........................]  Step: 98ms | Tot: 4s425ms | test_Loss: 4.155 | test_Acc: 13.659% (601/4400) 44/100 \n",
      ">>>best acc: 7.43, mean: 15.1, std: 4.03...........................]  Step: 97ms | Tot: 4s523ms | test_Loss: 4.160 | test_Acc: 13.622% (613/4500) 45/100 \n",
      ">>>best acc: 7.43, mean: 15.07, std: 3.99..........................]  Step: 96ms | Tot: 4s620ms | test_Loss: 4.148 | test_Acc: 13.848% (637/4600) 46/100 \n",
      ">>>best acc: 7.43, mean: 15.05, std: 3.95..........................]  Step: 99ms | Tot: 4s719ms | test_Loss: 4.145 | test_Acc: 14.106% (663/4700) 47/100 \n",
      ">>>best acc: 7.43, mean: 15.02, std: 3.91..........................]  Step: 113ms | Tot: 4s832ms | test_Loss: 4.157 | test_Acc: 13.833% (664/4800) 48/100 \n",
      ">>>best acc: 7.43, mean: 15.0, std: 3.88...........................]  Step: 96ms | Tot: 4s929ms | test_Loss: 4.154 | test_Acc: 13.837% (678/4900) 49/100 \n",
      ">>>best acc: 7.43, mean: 14.97, std: 3.84..........................]  Step: 98ms | Tot: 5s28ms | test_Loss: 4.156 | test_Acc: 13.720% (686/5000) 50/100 \n",
      ">>>best acc: 7.43, mean: 14.95, std: 3.81..........................]  Step: 95ms | Tot: 5s123ms | test_Loss: 4.153 | test_Acc: 13.706% (699/5100) 51/100 \n",
      ">>>best acc: 7.43, mean: 14.93, std: 3.77..........................]  Step: 95ms | Tot: 5s218ms | test_Loss: 4.148 | test_Acc: 13.923% (724/5200) 52/100 \n",
      ">>>best acc: 7.43, mean: 14.91, std: 3.74..........................]  Step: 105ms | Tot: 5s324ms | test_Loss: 4.152 | test_Acc: 13.792% (731/5300) 53/100 \n",
      ">>>best acc: 7.43, mean: 14.89, std: 3.71..........................]  Step: 105ms | Tot: 5s430ms | test_Loss: 4.144 | test_Acc: 13.889% (750/5400) 54/100 \n",
      ">>>best acc: 7.43, mean: 14.87, std: 3.68..........................]  Step: 96ms | Tot: 5s527ms | test_Loss: 4.145 | test_Acc: 13.782% (758/5500) 55/100 \n",
      ">>>best acc: 7.43, mean: 14.85, std: 3.65..........................]  Step: 97ms | Tot: 5s624ms | test_Loss: 4.137 | test_Acc: 13.875% (777/5600) 56/100 \n",
      ">>>best acc: 7.43, mean: 14.84, std: 3.62..........................]  Step: 98ms | Tot: 5s723ms | test_Loss: 4.129 | test_Acc: 14.140% (806/5700) 57/100 \n",
      ">>>best acc: 7.43, mean: 14.83, std: 3.59..........................]  Step: 97ms | Tot: 5s821ms | test_Loss: 4.119 | test_Acc: 14.241% (826/5800) 58/100 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>best acc: 7.43, mean: 14.82, std: 3.56..........................]  Step: 101ms | Tot: 5s922ms | test_Loss: 4.122 | test_Acc: 14.136% (834/5900) 59/100 \n",
      ">>>best acc: 7.43, mean: 14.81, std: 3.53..........................]  Step: 97ms | Tot: 6s20ms | test_Loss: 4.106 | test_Acc: 14.383% (863/6000) 60/100 \n",
      ">>>best acc: 7.43, mean: 14.8, std: 3.5==>.........................]  Step: 99ms | Tot: 6s119ms | test_Loss: 4.109 | test_Acc: 14.213% (867/6100) 61/100 \n",
      ">>>best acc: 7.43, mean: 14.79, std: 3.47>.........................]  Step: 100ms | Tot: 6s220ms | test_Loss: 4.123 | test_Acc: 13.984% (867/6200) 62/100 \n",
      ">>>best acc: 7.43, mean: 14.78, std: 3.45=>........................]  Step: 99ms | Tot: 6s320ms | test_Loss: 4.119 | test_Acc: 14.095% (888/6300) 63/100 \n",
      ">>>best acc: 7.43, mean: 14.76, std: 3.42=>........................]  Step: 96ms | Tot: 6s417ms | test_Loss: 4.124 | test_Acc: 13.891% (889/6400) 64/100 \n",
      ">>>best acc: 7.43, mean: 14.75, std: 3.4===>.......................]  Step: 98ms | Tot: 6s515ms | test_Loss: 4.120 | test_Acc: 13.831% (899/6500) 65/100 \n",
      ">>>best acc: 7.43, mean: 14.73, std: 3.37===>......................]  Step: 105ms | Tot: 6s621ms | test_Loss: 4.125 | test_Acc: 13.667% (902/6600) 66/100 \n",
      ">>>best acc: 7.43, mean: 14.71, std: 3.35===>......................]  Step: 99ms | Tot: 6s721ms | test_Loss: 4.125 | test_Acc: 13.567% (909/6700) 67/100 \n",
      ">>>best acc: 7.43, mean: 14.7, std: 3.33=====>.....................]  Step: 114ms | Tot: 6s835ms | test_Loss: 4.133 | test_Acc: 13.456% (915/6800) 68/100 \n",
      ">>>best acc: 7.43, mean: 14.68, std: 3.31=====>....................]  Step: 101ms | Tot: 6s937ms | test_Loss: 4.146 | test_Acc: 13.275% (916/6900) 69/100 \n",
      ">>>best acc: 7.43, mean: 14.65, std: 3.29=====>....................]  Step: 97ms | Tot: 7s34ms | test_Loss: 4.155 | test_Acc: 13.143% (920/7000) 70/100 \n",
      ">>>best acc: 7.43, mean: 14.63, std: 3.27======>...................]  Step: 102ms | Tot: 7s137ms | test_Loss: 4.164 | test_Acc: 12.986% (922/7100) 71/100 \n",
      ">>>best acc: 7.43, mean: 14.61, std: 3.26=======>..................]  Step: 95ms | Tot: 7s233ms | test_Loss: 4.165 | test_Acc: 13.000% (936/7200) 72/100 \n",
      ">>>best acc: 7.43, mean: 14.59, std: 3.24=======>..................]  Step: 95ms | Tot: 7s328ms | test_Loss: 4.162 | test_Acc: 13.027% (951/7300) 73/100 \n",
      ">>>best acc: 7.43, mean: 14.56, std: 3.22========>.................]  Step: 113ms | Tot: 7s442ms | test_Loss: 4.158 | test_Acc: 12.973% (960/7400) 74/100 \n",
      ">>>best acc: 7.43, mean: 14.54, std: 3.21=========>................]  Step: 104ms | Tot: 7s547ms | test_Loss: 4.171 | test_Acc: 12.813% (961/7500) 75/100 \n",
      ">>>best acc: 7.43, mean: 14.52, std: 3.19=========>................]  Step: 99ms | Tot: 7s647ms | test_Loss: 4.180 | test_Acc: 12.645% (961/7600) 76/100 \n",
      ">>>best acc: 7.43, mean: 14.49, std: 3.18==========>...............]  Step: 94ms | Tot: 7s742ms | test_Loss: 4.166 | test_Acc: 12.844% (989/7700) 77/100 \n",
      ">>>best acc: 7.43, mean: 14.47, std: 3.16===========>..............]  Step: 104ms | Tot: 7s846ms | test_Loss: 4.164 | test_Acc: 12.795% (998/7800) 78/100 \n",
      ">>>best acc: 7.43, mean: 14.45, std: 3.15===========>..............]  Step: 103ms | Tot: 7s949ms | test_Loss: 4.162 | test_Acc: 12.696% (1003/7900) 79/100 \n",
      ">>>best acc: 7.43, mean: 14.43, std: 3.14============>.............]  Step: 100ms | Tot: 8s50ms | test_Loss: 4.172 | test_Acc: 12.550% (1004/8000) 80/100 \n",
      ">>>best acc: 7.43, mean: 14.4, std: 3.13==============>............]  Step: 98ms | Tot: 8s148ms | test_Loss: 4.175 | test_Acc: 12.457% (1009/8100) 81/100 \n",
      ">>>best acc: 7.43, mean: 14.38, std: 3.11=============>............]  Step: 99ms | Tot: 8s248ms | test_Loss: 4.164 | test_Acc: 12.476% (1023/8200) 82/100 \n",
      ">>>best acc: 7.43, mean: 14.36, std: 3.1===============>...........]  Step: 96ms | Tot: 8s344ms | test_Loss: 4.153 | test_Acc: 12.663% (1051/8300) 83/100 \n",
      ">>>best acc: 7.43, mean: 14.34, std: 3.09==============>...........]  Step: 104ms | Tot: 8s448ms | test_Loss: 4.145 | test_Acc: 12.810% (1076/8400) 84/100 \n",
      ">>>best acc: 7.43, mean: 14.32, std: 3.07===============>..........]  Step: 105ms | Tot: 8s554ms | test_Loss: 4.153 | test_Acc: 12.659% (1076/8500) 85/100 \n",
      ">>>best acc: 7.43, mean: 14.3, std: 3.06=================>.........]  Step: 96ms | Tot: 8s651ms | test_Loss: 4.142 | test_Acc: 12.779% (1099/8600) 86/100 \n",
      ">>>best acc: 7.43, mean: 14.28, std: 3.05================>.........]  Step: 95ms | Tot: 8s746ms | test_Loss: 4.147 | test_Acc: 12.759% (1110/8700) 87/100 \n",
      ">>>best acc: 7.43, mean: 14.27, std: 3.03=================>........]  Step: 119ms | Tot: 8s865ms | test_Loss: 4.152 | test_Acc: 12.670% (1115/8800) 88/100 \n",
      ">>>best acc: 7.43, mean: 14.25, std: 3.02==================>.......]  Step: 96ms | Tot: 8s961ms | test_Loss: 4.145 | test_Acc: 12.674% (1128/8900) 89/100 \n",
      ">>>best acc: 7.43, mean: 14.23, std: 3.01==================>.......]  Step: 97ms | Tot: 9s58ms | test_Loss: 4.137 | test_Acc: 12.789% (1151/9000) 90/100 \n",
      ">>>best acc: 7.43, mean: 14.22, std: 3.0====================>......]  Step: 98ms | Tot: 9s157ms | test_Loss: 4.139 | test_Acc: 12.912% (1175/9100) 91/100 \n",
      ">>>best acc: 7.43, mean: 14.2, std: 2.98=====================>.....]  Step: 101ms | Tot: 9s258ms | test_Loss: 4.131 | test_Acc: 12.957% (1192/9200) 92/100 \n",
      ">>>best acc: 7.43, mean: 14.19, std: 2.97====================>.....]  Step: 101ms | Tot: 9s360ms | test_Loss: 4.126 | test_Acc: 13.054% (1214/9300) 93/100 \n",
      ">>>best acc: 7.43, mean: 14.18, std: 2.95=====================>....]  Step: 106ms | Tot: 9s466ms | test_Loss: 4.120 | test_Acc: 13.160% (1237/9400) 94/100 \n",
      ">>>best acc: 7.43, mean: 14.17, std: 2.94======================>...]  Step: 104ms | Tot: 9s571ms | test_Loss: 4.118 | test_Acc: 13.084% (1243/9500) 95/100 \n",
      ">>>best acc: 7.43, mean: 14.16, std: 2.93======================>...]  Step: 101ms | Tot: 9s673ms | test_Loss: 4.114 | test_Acc: 13.073% (1255/9600) 96/100 \n",
      ">>>best acc: 7.43, mean: 14.15, std: 2.91=======================>..]  Step: 103ms | Tot: 9s777ms | test_Loss: 4.107 | test_Acc: 13.227% (1283/9700) 97/100 \n",
      ">>>best acc: 7.43, mean: 14.14, std: 2.9=========================>.]  Step: 118ms | Tot: 9s895ms | test_Loss: 4.095 | test_Acc: 13.357% (1309/9800) 98/100 \n",
      ">>>best acc: 7.43, mean: 14.13, std: 2.89========================>.]  Step: 120ms | Tot: 10s16ms | test_Loss: 4.099 | test_Acc: 13.384% (1325/9900) 99/100 \n",
      " [================================================================>]  Step: 113ms | Tot: 10s129ms | test_Loss: 4.102 | test_Acc: 13.350% (1335/10000) 100/100 \n",
      ">>>best acc: 7.43, mean: 14.12, std: 2.87\n",
      "Saving..\n",
      ">>>best acc: 13.35\n",
      "Epoch:3/90\n",
      " [==================>..............................................]  Step: 153ms | Tot: 36s886ms | train_Loss: 3.839 | train_Acc: 15.327% (4353/28400) 284/1000 \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6e872a0bda4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mepoch_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0mtrain_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mtrain_acc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6e872a0bda4a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \"\"\"\n\u001b[1;32m    229\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maccimage_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpil_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAhH0lEQVR4nO3de3RU5b3/8fcXSEmjFEISLxBpor+uFrmFEigWLVCPysWqiFV/P1FrT0V7sXpqWWBFRGtblNayqBdEi8eKxRtFS6UKuILYVdFCTihYsOHWRYKnXISUCFGB7++P2dAhTMIkmT2TsD+vtWbNnv08e8/3SRZ8Zl/yjLk7IiISXe0yXYCIiGSWgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCIu1CAwsy1mtsbMKsxsZSP9BprZATO7Isx6RETkWB3S8B7D3X1nQ41m1h64H1iczM7y8/O9qKgoRaWJiETDqlWrdrp7QaK2dATB8dwCzAcGJtO5qKiIlSsbPLgQEZEEzOwfDbWFfY3AgcVmtsrMxtdvNLPuwBjg0cZ2YmbjzWylma3csWNHSKWKiERT2EFwrrt/ERgJfNfMvlKvfQYw0d0PNbYTd5/t7qXuXlpQkPDIRkREminUU0PuXh08bzezBcAgYHlcl1LgWTMDyAdGmdkBd38pzLpEROTfQgsCMzsJaOfue4PlC4F74/u4e3Fc//8G/qAQEImuTz75hKqqKurq6jJdSpuVnZ1NYWEhWVlZSW8T5hHBqcCC4NN+B+C37v6qmd0M4O6zQnxvEWmDqqqq6NSpE0VFRQT/d0gTuDu7du2iqqqK4uLi428QCC0I3H0T0C/B+oQB4O7fCKsWEWkb6urqFAItYGbk5eXR1Jtq9JfFItKqKARapjk/PwWBiEjEKQhERAJ79uzhkUceada2o0aNYs+ePUn3nzp1Kj//+c+b9V6ppiAQEQk0FgQHDhxodNtFixbRpUuXEKoKn4JARCQwadIkNm7cSElJCRMmTGDZsmWcd955XHLJJZx99tkAXHbZZQwYMIBevXoxe/bsI9sWFRWxc+dOtmzZQs+ePbnxxhvp1asXF154Ifv372/0fSsqKhg8eDB9+/ZlzJgx7N69G4CZM2dy9tln07dvX66++moA3njjDUpKSigpKaF///7s3bu3xeNuDXMNiYgc47bboKIitfssKYEZMxpunzZtGmvXrqUieONly5ZRXl7O2rVrj9yOOWfOHLp27cr+/fsZOHAgY8eOJS8v76j9VFZWMm/ePB5//HGuvPJK5s+fz7hx4xp83+uuu45f/epXDB06lClTpnDPPfcwY8YMpk2bxubNm+nYseOR004///nPefjhhxkyZAi1tbVkZ2e34CcSoyMCEZFGDBo06Kh78mfOnEm/fv0YPHgwW7dupbKy8phtiouLKSkpAWDAgAFs2bKlwf3X1NSwZ88ehg4dCsD111/P8uWxCRj69u3LNddcw9y5c+nQIfa5fciQIfzgBz9g5syZ7Nmz58j6ltARgYi0So19ck+nk0466cjysmXLWLp0KW+99RY5OTkMGzYs4V9Bd+zY8chy+/btj3tqqCGvvPIKy5cvZ+HChfzkJz9hzZo1TJo0idGjR7No0SKGDBnCa6+9xhe+8IVm7f8wHRGIiAQ6derU6Dn3mpoacnNzycnJYf369axYsaLF79m5c2dyc3N58803AXj66acZOnQohw4dYuvWrQwfPpz777+fmpoaamtr2bhxI3369GHixIkMHDiQ9evXt7gGHRGIiATy8vIYMmQIvXv3ZuTIkYwePfqo9hEjRjBr1ix69uzJ5z//eQYPHpyS933qqae4+eab2bdvH2eeeSZPPvkkBw8eZNy4cdTU1ODufP/736dLly7cddddlJWV0a5dO3r16sXIkSNb/P7m7ikYRvqUlpa6vphG5MS0bt06evbsmeky2rxEP0czW+XupYn669SQiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiIoGWTEMNMGPGDPbt25ewbdiwYbTWW98VBCIigTCDoDVTEIiIBOpPQw0wffp0Bg4cSN++fbn77rsB+PDDDxk9ejT9+vWjd+/ePPfcc8ycOZNt27YxfPhwhg8f3uj7zJs3jz59+tC7d28mTpwIwMGDB/nGN75B79696dOnD7/85S+BxFNRp5qmmBCR1ikD81DXn4Z68eLFVFZW8s477+DuXHLJJSxfvpwdO3bQrVs3XnnlFSA2B1Hnzp158MEHKSsrIz8/v8H32LZtGxMnTmTVqlXk5uZy4YUX8tJLL3HGGWdQXV3N2rVrAY5MO51oKupU0xGBiEgDFi9ezOLFi+nfvz9f/OIXWb9+PZWVlfTp04clS5YwceJE3nzzTTp37pz0Pv/yl78wbNgwCgoK6NChA9dccw3Lly/nzDPPZNOmTdxyyy28+uqrfOYznwEST0WdajoiEJHWqRXMQ+3u3HHHHdx0003HtJWXl7No0SImT57M+eefz5QpU1r0Xrm5uaxevZrXXnuNWbNm8fzzzzNnzpyEU1GnOhB0RCAiEqg/DfVFF13EnDlzqK2tBaC6uprt27ezbds2cnJyGDduHBMmTKC8vDzh9okMGjSIN954g507d3Lw4EHmzZvH0KFD2blzJ4cOHWLs2LHcd999lJeXNzgVdarpiEBEJFB/Gurp06ezbt06zjnnHABOPvlk5s6dy4YNG5gwYQLt2rUjKyuLRx99FIDx48czYsQIunXrRllZWcL3OP3005k2bRrDhw/H3Rk9ejSXXnopq1ev5oYbbuDQoUMA/OxnP2twKupU0zTUItJqaBrq1NA01CIi0iQKAhGRiFMQiEir0tZOV7c2zfn5KQhEpNXIzs5m165dCoNmcnd27dpFdnZ2k7YL9a4hM9sC7AUOAgfqX6gws2uAiYAF/b7t7qvDrElEWq/CwkKqqqrYsWNHpktps7KzsyksLGzSNum4fXS4u+9soG0zMNTdd5vZSGA28KU01CQirVBWVhbFxcWZLiNyMvp3BO7+57iXK4CmxZiIiLRY2NcIHFhsZqvMbPxx+v4n8MeQ6xERkXrCPiI4192rzewUYImZrXf35fU7mdlwYkFwbqKdBCEyHqBHjx5h1isiEjlJHRGY2a1m9hmL+bWZlZvZhcfbzt2rg+ftwAJgUIJ99wWeAC51910N7Ge2u5e6e2lBQUEyJYuISJKSPTX0TXf/F3AhkAtcC0xrbAMzO8nMOh1eDrZdW69PD+B3wLXu/vcm1i4iIimQ7KkhC55HAU+7+7tmZo1tAJwKLAi6dQB+6+6vmtnNAO4+C5gC5AGPBP2OucVURETClWwQrDKzxUAxcEfwSf9QYxu4+yagX4L1s+KWvwV8K/lyRUQk1ZINgv8ESoBN7r7PzLoCN4RWlYiIpE2y1wjOAd5z9z1mNg6YDNSEV5aIiKRLskHwKLDPzPoBtwMbgd+EVpWIiKRNskFwwGOzQF0KPOTuDwOdwitLRETSJdlrBHvN7A5it42eZ2btgKzwyhIRkXRJ9ojgKuAjYn9P8L/E5gSaHlpVIiKSNkkFQfCf/zNAZzO7GKhzd10jEBE5ASQ7xcSVwDvA14ErgbfN7IowCxMRkfRI9hrBncDAYM4gzKwAWAq8GFZhIiKSHsleI2h3OAQCu5qwrYiItGLJHhG8amavAfOC11cBi8IpSURE0impIHD3CWY2FhgSrJrt7gvCK0tERNIl6S+mcff5wPwQaxERkQxoNAjMbC+xr5s8pglwd/9MKFWJiEjaNBoE7q5pJERETnC680dEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYgLNQjMbIuZrTGzCjNbmaDdzGymmW0ws7+a2RfDrEdERI6V9FdVtsBwd9/ZQNtI4HPB40vAo8GziIikSaZPDV0K/MZjVgBdzOz0DNckIhIpYQeBA4vNbJWZjU/Q3h3YGve6KlgnIiJpEvapoXPdvdrMTgGWmNl6d1/e1J0EITIeoEePHqmuUUQk0kI9InD36uB5O7AAGFSvSzVwRtzrwmBd/f3MdvdSdy8tKCgIq1wRkUgKLQjM7CQz63R4GbgQWFuv2++B64K7hwYDNe7+flg1iYjIscI8NXQqsMDMDr/Pb939VTO7GcDdZwGLgFHABmAfcEOI9YiISAKhBYG7bwL6JVg/K27Zge+GVYOIiBxfpm8fFRGRDFMQiIhEnIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYk4i00A2naY2Q7gH5muoxnygZ2ZLiLNNOYTX9TGC213zJ9194Tf7NXmgqCtMrOV7l6a6TrSSWM+8UVtvHBijlmnhkREIk5BICIScQqC9Jmd6QIyQGM+8UVtvHACjlnXCEREIk5HBCIiEacgEBGJOAVBCplZVzNbYmaVwXNuA/2uD/pUmtn1Cdp/b2Zrw6+45VoyZjPLMbNXzGy9mb1rZtPSW33yzGyEmb1nZhvMbFKC9o5m9lzQ/raZFcW13RGsf8/MLkpr4S3Q3DGb2QVmtsrM1gTPX0178c3Ukt9z0N7DzGrN7IdpKzoV3F2PFD2AB4BJwfIk4P4EfboCm4Ln3GA5N679cuC3wNpMjyfsMQM5wPCgz6eAN4GRmR5TgvrbAxuBM4M6VwNn1+vzHWBWsHw18FywfHbQvyNQHOynfabHFPKY+wPdguXeQHWmxxP2mOPaXwReAH6Y6fE05aEjgtS6FHgqWH4KuCxBn4uAJe7+gbvvBpYAIwDM7GTgB8B94ZeaMs0es7vvc/cyAHf/GCgHCsMvuckGARvcfVNQ57PExh0v/ufwInC+mVmw/ll3/8jdNwMbgv21ds0es7v/j7tvC9a/C3zazDqmpeqWacnvGTO7DNhMbMxtioIgtU519/eD5f8FTk3QpzuwNe51VbAO4MfAL4B9oVWYei0dMwBm1gX4GvB6CDW21HHrj+/j7geAGiAvyW1bo5aMOd5YoNzdPwqpzlRq9piDD3ETgXvSUGfKdch0AW2NmS0FTkvQdGf8C3d3M0v63lwzKwHOcvf/qn/eMdPCGnPc/jsA84CZ7r6peVVKa2NmvYD7gQszXUsaTAV+6e61wQFCm6IgaCJ3/4+G2szsn2Z2uru/b2anA9sTdKsGhsW9LgSWAecApWa2hdjv5RQzW+buw8iwEMd82Gyg0t1ntLzaUFQDZ8S9LgzWJepTFQRbZ2BXktu2Ri0ZM2ZWCCwArnP3jeGXmxItGfOXgCvM7AGgC3DIzOrc/aHQq06FTF+kOJEewHSOvnD6QII+XYmdR8wNHpuBrvX6FNF2Lha3aMzErofMB9pleiyNjLEDsQvcxfz7ImKven2+y9EXEZ8Plntx9MXiTbSNi8UtGXOXoP/lmR5HusZcr89U2tjF4owXcCI9iJ0ffR2oBJbG/WdXCjwR1++bxC4abgBuSLCfthQEzR4zsU9cDqwDKoLHtzI9pgbGOQr4O7G7Su4M1t0LXBIsZxO7W2QD8A5wZty2dwbbvUcrvCsq1WMGJgMfxv1OK4BTMj2esH/Pcftoc0GgKSZERCJOdw2JiEScgkBEJOIUBCIiEdfmbh/Nz8/3oqKiTJchItKmrFq1aqc38J3FbS4IioqKWLlyZabLEBFpU8zsHw216dSQiEjEKQhERCJOQSAiEnFt7hqBiJz4PvnkE6qqqqirq8t0KW1OdnY2hYWFZGVlJb2NgkBEWp2qqio6depEUVERbXE2z0xxd3bt2kVVVRXFxcVJb6dTQyLS6tTV1ZGXl6cQaCIzIy8vr8lHUgoCEWmVFALN05yfm4JARCTiFAQiIvXs2bOHRx55pFnbjho1ij179qS2oJApCERE6mksCA4cONDotosWLaJLly4hVBUe3TUkIq3abbdBRUVq91lSAjNmNNw+adIkNm7cSElJCRdccAGjR4/mrrvuIjc3l/Xr1/P3v/+dyy67jK1bt1JXV8ett97K+PHjgX9Pg1NbW8vIkSM599xz+fOf/0z37t15+eWX+fSnP33Uey1cuJD77ruPjz/+mLy8PJ555hlOPfVUamtrueWWW1i5ciVmxt13383YsWN59dVX+dGPfsTBgwfJz8/n9ddfb/HPQ0EgIlLPtGnTWLt2LRVBAi1btozy8nLWrl175LbMOXPm0LVrV/bv38/AgQMZO3YseXl5R+2nsrKSefPm8fjjj3PllVcyf/58xo0bd1Sfc889lxUrVmBmPPHEEzzwwAP84he/4Mc//jGdO3dmzZo1AOzevZsdO3Zw4403snz5coqLi/nggw9SMl4FgYi0ao19ck+nQYMGHXVv/syZM1mwYAEAW7dupbKy8pggKC4upqSkBIABAwawZcuWY/ZbVVXFVVddxfvvv8/HH3985D2WLl3Ks88+e6Rfbm4uCxcu5Ctf+cqRPl27dk3J2HSNQEQkCSeddNKR5WXLlrF06VLeeustVq9eTf/+/RPeu9+xY8cjy+3bt094feGWW27he9/7HmvWrOGxxx7LyF9TKwhEROrp1KkTe/fubbC9pqaG3NxccnJyWL9+PStWrGj2e9XU1NC9e3cAnnrqqSPrL7jgAh5++OEjr3fv3s3gwYNZvnw5mzdvBkjZqSEFgYhIPXl5eQwZMoTevXszYcKEY9pHjBjBgQMH6NmzJ5MmTWLw4MHNfq+pU6fy9a9/nQEDBpCfn39k/eTJk9m9eze9e/emX79+lJWVUVBQwOzZs7n88svp168fV111VbPfN565e0p2lC6lpaWuL6YRObGtW7eOnj17ZrqMNivRz8/MVrl7aaL+OiIQEYk4BYGISMSFFgRmdoaZlZnZ38zsXTO7NUGfYWZWY2YVwWNKWPWIiEhiYf4dwQHgdncvN7NOwCozW+Luf6vX7013vzjEOkREpBGhHRG4+/vuXh4s7wXWAd3Dej8REWmetFwjMLMioD/wdoLmc8xstZn90cx6NbD9eDNbaWYrd+zYEWapIiKRE3oQmNnJwHzgNnf/V73mcuCz7t4P+BXwUqJ9uPtsdy9199KCgoJQ6xURack01AAzZsxg3759KawoXKEGgZllEQuBZ9z9d/Xb3f1f7l4bLC8Csswsv34/EZF0iloQhHax2GLfl/ZrYJ27P9hAn9OAf7q7m9kgYsG0K6yaRKQNysA81PWnoZ4+fTrTp0/n+eef56OPPmLMmDHcc889fPjhh1x55ZVUVVVx8OBB7rrrLv75z3+ybds2hg8fTn5+PmVlZUft+95772XhwoXs37+fL3/5yzz22GOYGRs2bODmm29mx44dtG/fnhdeeIGzzjqL+++/n7lz59KuXTtGjhzJtGnTUvuzINy7hoYA1wJrzKwiWPcjoAeAu88CrgC+bWYHgP3A1d7W/tRZRE449aehXrx4MZWVlbzzzju4O5dccgnLly9nx44ddOvWjVdeeQWIzRvUuXNnHnzwQcrKyo6aMuKw733ve0yZErtT/tprr+UPf/gDX/va17jmmmuYNGkSY8aMoa6ujkOHDvHHP/6Rl19+mbfffpucnJyUzS1UX2hB4O5/Ahr9FmV3fwh4KKwaROQE0ArmoV68eDGLFy+mf//+ANTW1lJZWcl5553H7bffzsSJE7n44os577zzjruvsrIyHnjgAfbt28cHH3xAr169GDZsGNXV1YwZMwaA7OxsIDYV9Q033EBOTg6Qummn69P3EYiIHIe7c8cdd3DTTTcd01ZeXs6iRYuYPHky559//pFP+4nU1dXxne98h5UrV3LGGWcwderUjEw7XZ+mmBARqaf+NNQXXXQRc+bMoba2FoDq6mq2b9/Otm3byMnJYdy4cUyYMIHy8vKE2x92+D/9/Px8amtrefHFF4/0Lyws5KWXXgLgo48+Yt++fVxwwQU8+eSTRy48t7lTQyIibVX8NNQjR45k+vTprFu3jnPOOQeAk08+mblz57JhwwYmTJhAu3btyMrK4tFHHwVg/PjxjBgxgm7duh11sbhLly7ceOON9O7dm9NOO42BAwceaXv66ae56aabmDJlCllZWbzwwguMGDGCiooKSktL+dSnPsWoUaP46U9/mvLxahpqEWl1NA11y2gaahERaRIFgYhIxCkIRKRVamunrVuL5vzcFAQi0upkZ2eza9cuhUETuTu7du068ncIydJdQyLS6hQWFlJVVYVmG2667OxsCgsLm7SNgkBEWp2srCyKi4szXUZk6NSQiEjEKQhERCJOQSAiEnEKAhGRiEsqCMxsjJl1jnvdxcwuC60qERFJm2SPCO5295rDL9x9D3B3KBWJiEhaJRsEifrp1lMRkRNAskGw0sweNLOzgseDwKowCxMRkfRINghuAT4GngOeBeqA74ZVlIiIpE9Sp3fc/UNgUsi1iIhIBiR719ASM+sS9zrXzF4LrSoREUmbZE8N5Qd3CgHg7ruBU0KpSERE0irZIDhkZj0OvzCzIqDR+WHN7AwzKzOzv5nZu2Z2a4I+ZmYzzWyDmf3VzL7YpOpFRKTFkr0F9E7gT2b2BmDAecD442xzALjd3cvNrBOwysyWuPvf4vqMBD4XPL4EPBo8i4hImiR1RODurwKlwHvAPOB2YP9xtnnf3cuD5b3AOqB7vW6XAr/xmBVAFzM7vWlDEBGRlkjqiMDMvgXcChQCFcBg4C3gq0luXwT0B96u19Qd2Br3uipY93697ccTHIH06NEDERFJnWSvEdwKDAT+4e7Dif2nvieZDc3sZGA+cJu7/6s5Rbr7bHcvdffSgoKC5uxCREQakGwQ1Ll7HYCZdXT39cDnj7eRmWURC4Fn3P13CbpUA2fEvS4M1omISJokGwRVwd8RvAQsMbOXgX80toGZGfBrYJ27P9hAt98D1wV3Dw0Gatz9/Qb6iohICJL9y+IxweJUMysDOgOvHmezIcC1wBozqwjW/QjoEexzFrAIGAVsAPYBNzSleBERabkmzyDq7m8k2e9PxG41bayPozmLREQySt9QJiIScQoCEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBiEjEKQhERCJOQSAiEnEKAhGRiFMQiIhEnIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJxCgIRkYhTEIiIRJyCQEQk4hQEIiIRpyAQEYm40ILAzOaY2XYzW9tA+zAzqzGziuAxJaxaRESkYR1C3Pd/Aw8Bv2mkz5vufnGINYiIyHGEdkTg7suBD8Lav4iIpEamrxGcY2arzeyPZtYrw7WIiERSmKeGjqcc+Ky715rZKOAl4HOJOprZeGA8QI8ePdJWoIhIFGTsiMDd/+XutcHyIiDLzPIb6Dvb3UvdvbSgoCCtdYqInOgyFgRmdpqZWbA8KKhlV6bqERGJqtBODZnZPGAYkG9mVcDdQBaAu88CrgC+bWYHgP3A1e7uYdUjIiKJhRYE7v5/j9P+ELHbS0VEJIMyfdeQiIhkmIJARCTiFAQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJx1tb+hsvMdgD/yHQdzZAP7Mx0EWmmMZ/4ojZeaLtj/qy7J5yjp80FQVtlZivdvTTTdaSTxnzii9p44cQcs04NiYhEnIJARCTiFATpMzvTBWSAxnzii9p44QQcs64RiIhEnI4IREQiTkEgIhJxCoIUMrOuZrbEzCqD59wG+l0f9Kk0s+sTtP/ezNaGX3HLtWTMZpZjZq+Y2Xoze9fMpqW3+uSZ2Qgze8/MNpjZpATtHc3suaD9bTMrimu7I1j/npldlNbCW6C5YzazC8xslZmtCZ6/mvbim6klv+egvYeZ1ZrZD9NWdCq4ux4pegAPAJOC5UnA/Qn6dAU2Bc+5wXJuXPvlwG+BtZkeT9hjBnKA4UGfTwFvAiMzPaYE9bcHNgJnBnWuBs6u1+c7wKxg+WrguWD57KB/R6A42E/7TI8p5DH3B7oFy72B6kyPJ+wxx7W/CLwA/DDT42nKQ0cEqXUp8FSw/BRwWYI+FwFL3P0Dd98NLAFGAJjZycAPgPvCLzVlmj1md9/n7mUA7v4xUA4Uhl9ykw0CNrj7pqDOZ4mNO178z+FF4PzgO7kvBZ5194/cfTOwIdhfa9fsMbv7/7j7tmD9u8CnzaxjWqpumZb8njGzy4DNxMbcpigIUutUd38/WP5f4NQEfboDW+NeVwXrAH4M/ALYF1qFqdfSMQNgZl2ArwGvh1BjSx23/vg+7n4AqAHykty2NWrJmOONBcrd/aOQ6kylZo85+BA3EbgnDXWmXGjfWXyiMrOlwGkJmu6Mf+HubmZJ35trZiXAWe7+X/XPO2ZaWGOO238HYB4w0903Na9KaW3MrBdwP3BhpmtJg6nAL929NjhAaFMUBE3k7v/RUJuZ/dPMTnf3983sdGB7gm7VwLC414XAMuAcoNTMthD7vZxiZsvcfRgZFuKYD5sNVLr7jJZXG4pq4Iy414XBukR9qoJg6wzsSnLb1qglY8bMCoEFwHXuvjH8clOiJWP+EnCFmT0AdAEOmVmduz8UetWpkOmLFCfSA5jO0RdOH0jQpyux84i5wWMz0LVenyLazsXiFo2Z2PWQ+UC7TI+lkTF2IHaBu5h/X0TsVa/Pdzn6IuLzwXIvjr5YvIm2cbG4JWPuEvS/PNPjSNeY6/WZShu7WJzxAk6kB7Hzo68DlcDSuP/sSoEn4vp9k9hFww3ADQn205aCoNljJvaJy4F1QEXw+Famx9TAOEcBfyd2V8mdwbp7gUuC5Wxid4tsAN4Bzozb9s5gu/dohXdFpXrMwGTgw7jfaQVwSqbHE/bvOW4fbS4INMWEiEjE6a4hEZGIUxCIiEScgkBEJOIUBCIiEacgEBGJOAWBSBqZ2TAz+0Om6xCJpyAQEYk4BYFIAmY2zszeMbMKM3vMzNoH88z/MvjuhNfNrCDoW2JmK8zsr2a24PB3MpjZ/zGzpWa22szKzeysYPcnm9mLwfcwPHN49kqRTFEQiNRjZj2Bq4Ah7l4CHASuAU4CVrp7L+AN4O5gk98AE929L7Ambv0zwMPu3g/4MnB4ltb+wG3EvqvgTGBIyEMSaZQmnRM51vnAAOAvwYf1TxObTO8Q8FzQZy7wOzPrDHRx9zeC9U8BL5hZJ6C7uy8AcPc6gGB/77h7VfC6gtiUIn8KfVQiDVAQiBzLgKfc/Y6jVprdVa9fc+dniZ+b/yD6dygZplNDIsd6ndiUwqfAke9l/iyxfy9XBH3+H/And68BdpvZecH6a4E33H0vsamKLwv20dHMctI5CJFk6ZOISD3u/jczmwwsNrN2wCfEph/+EBgUtG0ndh0B4HpgVvAf/SbghmD9tcBjZnZvsI+vp3EYIknT7KMiSTKzWnc/OdN1iKSaTg2JiEScjghERCJORwQiIhGnIBARiTgFgYhIxCkIREQiTkEgIhJx/x9IIpX+8LGF3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "wandb.watch(net)\n",
    "\n",
    "\n",
    "epochs = 90\n",
    "def train(epoch):\n",
    "    print('Epoch:{0}/{1}'.format(epoch, epochs))\n",
    "    net.train()\n",
    "    \n",
    "    train_loss = 0 \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)        \n",
    "        loss = criterion(outputs, targets)       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        progress_bar(batch_idx, len(trainloader), 'train_Loss: %.3f | train_Acc: %.3f%% (%d/%d)'\n",
    "                     % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "\n",
    "    return train_loss/(batch_idx+1), 100.*correct/total\n",
    "\n",
    "def test(epoch):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    acc_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            acc_list.append(100.*correct/total)\n",
    "            \n",
    "            progress_bar(batch_idx, len(testloader), 'test_Loss: %.3f | test_Acc: %.3f%% (%d/%d)'\n",
    "                         % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            print('>>>best acc: {0}, mean: {1}, std: {2}'.format(best_acc, round(np.mean(acc_list), 2), round(np.std(acc_list), 2)))\n",
    "            \n",
    "    # Save checkpoint.\n",
    "    acc = 100.*correct/total\n",
    "    if acc > best_acc:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        if not os.path.isdir(save_path+'/checkpoint'):\n",
    "            os.mkdir(save_path+'/checkpoint')\n",
    "        torch.save(state, save_path+'/checkpoint/ckpt.pth')\n",
    "        best_acc = acc\n",
    "        print('>>>best acc:', best_acc)\n",
    "    \n",
    "    return test_loss/(batch_idx+1), 100.*correct/total, best_acc\n",
    "\n",
    "test_loss = 0\n",
    "test_list = []\n",
    "train_list = []\n",
    "epoch_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "for epoch in range(start_epoch, start_epoch+epochs):\n",
    "   \n",
    "    epoch_list.append(epoch)\n",
    "    \n",
    "    train_loss, train_acc = train(epoch)\n",
    "    train_list.append(train_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    \n",
    "    test_loss, test_acc, best_acc = test(epoch)\n",
    "    test_list.append(test_loss)\n",
    "    test_acc_list.append(test_acc)\n",
    "    \n",
    "    epoch_line = 'epoch: {0}/ total epoch: {1} '.format(epoch, epochs) \n",
    "    best_acc_line = 'best_acc: {0} '.format(best_acc)\n",
    "    accuracy_line = 'train_acc: {0} %, test_acc: {1} % '.format(train_acc, test_acc)\n",
    "    loss_line = 'train_loss: {0},e test_loss: {1} '.format(train_loss, test_loss)\n",
    "    wandb.log({\"train accuracy\" : train_acc, \"test accuracy\" : test_acc, \"train_loss\" : train_loss, \"test_loss\" : test_loss}, step=epoch+1)\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        plt.figure()\n",
    "        ax1 = plt.subplot(2, 1, 1)\n",
    "        ax1.plot(epoch_list, train_list, c = 'blue', label = 'train loss')\n",
    "        ax1.plot(epoch_list, test_list, c = 'red', label = 'test loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        ax1.legend(loc=0)\n",
    "        \n",
    "        ax2 = plt.subplot(2, 1, 2)\n",
    "        ax2.plot(epoch_list, train_acc_list, c = 'blue', label = 'train acc')\n",
    "        ax2.plot(epoch_list, test_acc_list, c = 'red', label = 'test acc')\n",
    "        plt.ylabel('acc')\n",
    "        plt.xlabel('epoch')\n",
    "        ax2.legend(loc=0)\n",
    "        \n",
    "        plt.savefig(save_path+'/train_history.png')\n",
    "\n",
    "    \n",
    "    with open(save_path+'/logs.txt', 'a') as f:\n",
    "        f.write(epoch_line + best_acc_line + accuracy_line + loss_line + '\\n')\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
